{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Implementation\n",
    "\n",
    "Complete implementation guide covering:\n",
    "- Binary & Multi-class Classification\n",
    "- Hyperparameter Tuning (GridSearchCV & RandomizedSearchCV)\n",
    "- Imbalanced Dataset Handling\n",
    "- ROC Curve & Threshold Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation with sklearn\n",
    "\n",
    "### Using `make_classification`\n",
    "\n",
    "> **Important:** `make_classification` creates pre-standardized data, skipping preprocessing steps.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `n_samples`: Number of data points\n",
    "- `n_features`: Number of independent features\n",
    "- `n_classes`: Number of output categories (2 for binary)\n",
    "- `n_informative`: Number of informative features\n",
    "- `n_redundant`: Number of redundant features\n",
    "- `random_state`: For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 10)\n",
      "y shape: (1000,)\n",
      "Unique classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import dataset creation tool\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,        # number of data points\n",
    "    n_features=10,         # number of independent features\n",
    "    n_classes=2,           # binary classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Unique classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964799</td>\n",
       "      <td>-0.066449</td>\n",
       "      <td>0.986768</td>\n",
       "      <td>-0.358079</td>\n",
       "      <td>0.997266</td>\n",
       "      <td>1.181890</td>\n",
       "      <td>-1.615679</td>\n",
       "      <td>-1.210161</td>\n",
       "      <td>-0.628077</td>\n",
       "      <td>1.227274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.916511</td>\n",
       "      <td>-0.566395</td>\n",
       "      <td>-1.008614</td>\n",
       "      <td>0.831617</td>\n",
       "      <td>-1.176962</td>\n",
       "      <td>1.820544</td>\n",
       "      <td>1.752375</td>\n",
       "      <td>-0.984534</td>\n",
       "      <td>0.363896</td>\n",
       "      <td>0.209470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109484</td>\n",
       "      <td>-0.432774</td>\n",
       "      <td>-0.457649</td>\n",
       "      <td>0.793818</td>\n",
       "      <td>-0.268646</td>\n",
       "      <td>-1.836360</td>\n",
       "      <td>1.239086</td>\n",
       "      <td>-0.246383</td>\n",
       "      <td>-1.058145</td>\n",
       "      <td>-0.297376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.750412</td>\n",
       "      <td>2.023606</td>\n",
       "      <td>1.688159</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>-1.607661</td>\n",
       "      <td>0.184741</td>\n",
       "      <td>-2.619427</td>\n",
       "      <td>-0.357445</td>\n",
       "      <td>-1.473127</td>\n",
       "      <td>-0.190039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.224726</td>\n",
       "      <td>-0.711303</td>\n",
       "      <td>-0.220778</td>\n",
       "      <td>0.117124</td>\n",
       "      <td>1.536061</td>\n",
       "      <td>0.597538</td>\n",
       "      <td>0.348645</td>\n",
       "      <td>-0.939156</td>\n",
       "      <td>0.175915</td>\n",
       "      <td>0.236224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.964799 -0.066449  0.986768 -0.358079  0.997266  1.181890 -1.615679   \n",
       "1 -0.916511 -0.566395 -1.008614  0.831617 -1.176962  1.820544  1.752375   \n",
       "2 -0.109484 -0.432774 -0.457649  0.793818 -0.268646 -1.836360  1.239086   \n",
       "3  1.750412  2.023606  1.688159  0.006800 -1.607661  0.184741 -2.619427   \n",
       "4 -0.224726 -0.711303 -0.220778  0.117124  1.536061  0.597538  0.348645   \n",
       "\n",
       "          7         8         9  \n",
       "0 -1.210161 -0.628077  1.227274  \n",
       "1 -0.984534  0.363896  0.209470  \n",
       "2 -0.246383 -1.058145 -0.297376  \n",
       "3 -0.357445 -1.473127 -0.190039  \n",
       "4 -0.939156  0.175915  0.236224  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data as DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "0    501\n",
      "1    499\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check target distribution\n",
    "print(\"Class distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "> **Note:** `random_state` ensures same split every time for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 700\n",
      "Testing samples: 300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.30, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression Implementation\n",
    "\n",
    "**Key Parameters in LogisticRegression:**\n",
    "- `penalty`: L1, L2, elasticnet, none (regularization type)\n",
    "- `C`: Inverse of regularization strength (higher C = less regularization)\n",
    "- `solver`: Algorithm to use (newton-cg, lbfgs, liblinear, sag, saga)\n",
    "- `class_weight`: For handling imbalanced datasets\n",
    "- `max_iter`: Maximum iterations for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train model\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "> **Remember:** `predict()` gives class labels (0 or 1), while `predict_proba()` gives probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (first 10):\n",
      "[0 1 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels\n",
    "y_pred = logistic.predict(X_test)\n",
    "print(\"Predictions (first 10):\")\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities (first 5):\n",
      "[[0.77447791 0.22552209]\n",
      " [0.0336685  0.9663315 ]\n",
      " [0.67068215 0.32931785]\n",
      " [0.0798668  0.9201332 ]\n",
      " [0.97661665 0.02338335]]\n",
      "\n",
      "Columns: [P(class=0), P(class=1)]\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "y_pred_proba = logistic.predict_proba(X_test)\n",
    "print(\"Probabilities (first 5):\")\n",
    "print(y_pred_proba[:5])\n",
    "print(\"\\nColumns: [P(class=0), P(class=1)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "**Key Metrics:**\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Samples}}$$\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8467\n",
      "Accuracy Percentage: 84.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[118  17]\n",
      " [ 29 136]]\n",
      "\n",
      "[[TN, FP]\n",
      " [FN, TP]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\n[[TN, FP]\")\n",
    "print(\" [FN, TP]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84       135\n",
      "           1       0.89      0.82      0.86       165\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.85      0.85      0.85       300\n",
      "weighted avg       0.85      0.85      0.85       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "### Method 1: GridSearchCV\n",
    "\n",
    "> **Definition:** GridSearchCV tests ALL combinations of parameters to find the best one.\n",
    "\n",
    "> **Warning:** Can be time-consuming with large parameter grids.\n",
    "\n",
    "**Strategy:**\n",
    "1. Define parameter grid\n",
    "2. Use cross-validation (StratifiedKFold)\n",
    "3. Find best combination based on scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Grid:\n",
      "penalty: ['l1', 'l2', 'elasticnet', 'none']\n",
      "C: [100, 10, 1.0, 0.1, 0.01]\n",
      "solver: ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Define parameter grid\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [100, 10, 1.0, 0.1, 0.01],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "print(\"Parameter Grid:\")\n",
    "for key, values in params.items():\n",
    "    print(f\"{key}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    scoring='accuracy',\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=False),\n",
    "    n_jobs=-1,  # use all processors\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"GridSearchCV initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:490: FitFailedWarning: \n",
      "300 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 73, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "23 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1137: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.87              nan 0.87       0.87\n",
      " 0.87       0.87       0.87       0.87              nan        nan\n",
      "        nan        nan 0.87              nan        nan        nan\n",
      "        nan        nan        nan        nan 0.87              nan\n",
      " 0.87       0.87       0.87       0.87       0.87       0.87\n",
      "        nan        nan        nan        nan 0.87              nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87              nan 0.87       0.87       0.87       0.87\n",
      " 0.87       0.87              nan        nan        nan        nan\n",
      " 0.87              nan        nan        nan        nan        nan\n",
      "        nan        nan 0.87428571        nan 0.87       0.87285714\n",
      " 0.87285714 0.87428571 0.87285714 0.87285714        nan        nan\n",
      "        nan        nan 0.87285714        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.87              nan\n",
      " 0.87       0.87857143 0.87857143 0.87714286 0.87857143 0.87714286\n",
      "        nan        nan        nan        nan 0.87857143        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit GridSearchCV (this will take time)\n",
    "print(\"Starting GridSearchCV...\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "\n",
      "Best Cross-Validation Score: 0.8786\n"
     ]
    }
   ],
   "source": [
    "# Best parameters and score\n",
    "print(\"Best Parameters:\")\n",
    "print(grid.best_params_)\n",
    "print(f\"\\nBest Cross-Validation Score: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "Accuracy: 0.8533\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       135\n",
      "           1       0.92      0.80      0.86       165\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.86      0.86      0.85       300\n",
      "weighted avg       0.86      0.85      0.85       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with best model\n",
    "y_pred_grid = grid.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_grid):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: RandomizedSearchCV\n",
    "\n",
    "> **Tip:** RandomizedSearchCV picks random parameter combinations, making it faster than GridSearchCV.\n",
    "\n",
    "**Advantage:** Much faster when you have large parameter spaces.\n",
    "\n",
    "| Method | Speed | Coverage |\n",
    "|--------|-------|----------|\n",
    "| GridSearchCV | Slower | Tests ALL combinations |\n",
    "| RandomizedSearchCV | Faster | Tests RANDOM sample |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV initialized!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_cv = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=params,\n",
    "    n_iter=20,  # number of random combinations to try\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:490: FitFailedWarning: \n",
      "80 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 73, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1147, in fit\n",
      "    solver = _check_solver(self.solver, penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 65, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1137: UserWarning: One or more of the test scores are non-finite: [ nan  nan  nan 0.87 0.87  nan 0.87  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan 0.87  nan  nan  nan  nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\Desktop\\Data analyst bootcamp\\Python\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit RandomizedSearchCV\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "random_cv.fit(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'solver': 'newton-cg', 'penalty': 'l2', 'C': 1.0}\n",
      "\n",
      "Best Cross-Validation Score: 0.8700\n"
     ]
    }
   ],
   "source": [
    "# Best parameters and score\n",
    "print(\"Best Parameters:\")\n",
    "print(random_cv.best_params_)\n",
    "print(f\"\\nBest Cross-Validation Score: {random_cv.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "Accuracy: 0.8467\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84       135\n",
      "           1       0.89      0.82      0.86       165\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.85      0.85      0.85       300\n",
      "weighted avg       0.85      0.85      0.85       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with best model\n",
    "y_pred_random = random_cv.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_random):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-class Classification\n",
    "\n",
    "### One-vs-Rest (OVR) Approach\n",
    "\n",
    "> **Important:** In OVR, for N classes, N separate binary classification models are created internally.\n",
    "\n",
    "**`multi_class` parameter options:**\n",
    "- `'auto'`: Default, automatically selects\n",
    "- `'ovr'`: One-vs-Rest (creates N binary models for N classes)\n",
    "- `'multinomial'`: Minimizes multinomial loss across entire probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2]\n",
      "\n",
      "Class distribution:\n",
      "0    331\n",
      "1    334\n",
      "2    335\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create multi-class dataset (3 classes)\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_classes=3,        # 3 categories\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Unique classes: {np.unique(y_multi)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(pd.Series(y_multi).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, \n",
    "    test_size=0.30, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticRegression.__init__() got an unexpected keyword argument 'multi_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train model with OVR\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m logistic_multi = \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43movr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m logistic_multi.fit(X_train_multi, y_train_multi)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMulti-class model trained!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: LogisticRegression.__init__() got an unexpected keyword argument 'multi_class'"
     ]
    }
   ],
   "source": [
    "# Train model with OVR\n",
    "logistic_multi = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "logistic_multi.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "print(\"Multi-class model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_multi = logistic_multi.predict(X_test_multi)\n",
    "\n",
    "print(\"Predictions (first 10):\")\n",
    "print(y_pred_multi[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multi-class model\n",
    "accuracy_multi = accuracy_score(y_test_multi, y_pred_multi)\n",
    "print(f\"Accuracy: {accuracy_multi:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (3x3):\")\n",
    "cm_multi = confusion_matrix(y_test_multi, y_pred_multi)\n",
    "print(cm_multi)\n",
    "print(\"\\nDiagonal = Correct predictions\")\n",
    "print(\"Off-diagonal = Misclassifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-class confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.title('Multi-class Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imbalanced Dataset Handling\n",
    "\n",
    "> **Definition:** Imbalanced dataset = when one class has significantly more samples than others (e.g., 99:1 ratio)\n",
    "\n",
    "### Creating Imbalanced Dataset\n",
    "\n",
    "**Key Parameter:** `weights=[0.99]` creates 99:1 ratio between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create highly imbalanced dataset\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.99],  # 99% will be one class\n",
    "    flip_y=0,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "print(\"Class distribution:\")\n",
    "print(Counter(y_imb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imbalanced data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_imb[:, 0], y=X_imb[:, 1], hue=y_imb, palette='viridis', alpha=0.6)\n",
    "plt.title('Imbalanced Dataset Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(title='Class', labels=['Class 0 (Majority)', 'Class 1 (Minority)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for imbalanced data\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, \n",
    "    test_size=0.25, \n",
    "    random_state=42,\n",
    "    stratify=y_imb  # maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"Train set distribution:\")\n",
    "print(Counter(y_train_imb))\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(Counter(y_test_imb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `class_weight` Parameter\n",
    "\n",
    "> **Important:** `class_weight` assigns different importance to different classes during training.\n",
    "\n",
    "**Strategies:**\n",
    "- `{0: 1, 1: 1}` - Equal importance\n",
    "- `{0: 1, 1: 10}` - 10x more importance to class 1\n",
    "- `{0: 1, 1: 50}` - 50x more importance to class 1\n",
    "- `'balanced'` - Automatically adjust weights inversely proportional to class frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different class weight scenarios\n",
    "class_weight_options = [\n",
    "    {0: 1, 1: 1},   # equal importance\n",
    "    {0: 1, 1: 10},  # 10x more importance to minority class\n",
    "    {0: 1, 1: 50},  # 50x more importance\n",
    "    {0: 1, 1: 100}, # 100x more importance\n",
    "]\n",
    "\n",
    "print(\"Class weight options:\")\n",
    "for i, cw in enumerate(class_weight_options):\n",
    "    print(f\"{i+1}. {cw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for imbalanced data\n",
    "params_imb = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [100, 10, 1.0, 0.1, 0.01],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': class_weight_options\n",
    "}\n",
    "\n",
    "print(\"Parameter grid created for imbalanced data handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV for imbalanced data\n",
    "model_imb = LogisticRegression(max_iter=1000)\n",
    "\n",
    "grid_imb = GridSearchCV(\n",
    "    estimator=model_imb,\n",
    "    param_grid=params_imb,\n",
    "    scoring='accuracy',\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for imbalanced data...\")\n",
    "grid_imb.fit(X_train_imb, y_train_imb)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters for imbalanced data\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_imb.best_params_)\n",
    "print(f\"\\nBest CV Score: {grid_imb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_pred_imb = grid_imb.predict(X_test_imb)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_imb):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_imb, y_pred_imb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_imb, y_pred_imb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ROC Curve and Threshold Selection\n",
    "\n",
    "### Understanding Default Threshold\n",
    "\n",
    "> **Remember:** Default threshold = 0.5\n",
    "> - Probability  0.5  Class 1\n",
    "> - Probability < 0.5  Class 0\n",
    "\n",
    "**Problem:** Default 0.5 may not be optimal for all use cases (healthcare, finance, etc.)\n",
    "\n",
    "### ROC Curve Components\n",
    "\n",
    "$$\\text{False Positive Rate (FPR)} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$$\n",
    "\n",
    "$$\\text{True Positive Rate (TPR)} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "> **Definition:** TPR is also called **Sensitivity** or **Recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Create fresh dataset for ROC demonstration\n",
    "X_roc, y_roc = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_roc, X_test_roc, y_train_roc, y_test_roc = train_test_split(\n",
    "    X_roc, y_roc, test_size=0.30, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dummy Model for Comparison\n",
    "\n",
    "> **Note:** Dummy model always predicts probability = 0 (baseline for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy model: always predicts 0\n",
    "dummy_prob = [0 for _ in range(len(y_test_roc))]\n",
    "\n",
    "print(f\"Dummy predictions (first 10): {dummy_prob[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train actual logistic regression model\n",
    "model_roc = LogisticRegression(max_iter=1000)\n",
    "model_roc.fit(X_train_roc, y_train_roc)\n",
    "\n",
    "# Get probability predictions for positive class (class 1)\n",
    "model_prob = model_roc.predict_proba(X_test_roc)[:, 1]\n",
    "\n",
    "print(\"Model probabilities (first 10):\")\n",
    "print(model_prob[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ROC-AUC Scores\n",
    "\n",
    "**AUC Score Interpretation:**\n",
    "\n",
    "| AUC Score | Performance |\n",
    "|-----------|-------------|\n",
    "| 0.5 | Random (no discrimination) |\n",
    "| 0.5 - 0.7 | Poor |\n",
    "| 0.7 - 0.8 | Acceptable |\n",
    "| 0.8 - 0.9 | Excellent |\n",
    "| 0.9 - 1.0 | Outstanding |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC scores\n",
    "dummy_auc = roc_auc_score(y_test_roc, dummy_prob)\n",
    "model_auc = roc_auc_score(y_test_roc, model_prob)\n",
    "\n",
    "print(f\"Dummy Model AUC: {dummy_auc:.4f}\")\n",
    "print(f\"Logistic Regression AUC: {model_auc:.4f}\")\n",
    "print(f\"\\nImprovement: {(model_auc - dummy_auc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ROC Curve Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve for dummy model\n",
    "dummy_fpr, dummy_tpr, _ = roc_curve(y_test_roc, dummy_prob)\n",
    "\n",
    "# Calculate ROC curve for actual model\n",
    "model_fpr, model_tpr, thresholds = roc_curve(y_test_roc, model_prob)\n",
    "\n",
    "print(f\"Number of threshold points: {len(thresholds)}\")\n",
    "print(f\"Thresholds range: {thresholds.min():.4f} to {thresholds.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot basic ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Dummy model (diagonal line)\n",
    "plt.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model (AUC = 0.5)', color='blue')\n",
    "\n",
    "# Actual model\n",
    "plt.plot(model_fpr, model_tpr, marker='.', label=f'Logistic Regression (AUC = {model_auc:.2f})', color='orange')\n",
    "\n",
    "# Plot formatting\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve Comparison', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve with Threshold Annotations\n",
    "\n",
    "> **Important:** Each point on the ROC curve represents a different threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve with threshold annotations\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Dummy model\n",
    "plt.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model', color='blue')\n",
    "\n",
    "# Actual model\n",
    "plt.plot(model_fpr, model_tpr, marker='.', label='Logistic Regression', color='orange')\n",
    "\n",
    "# Annotate thresholds on the curve\n",
    "for i, threshold in enumerate(thresholds[::10]):  # annotate every 10th point to avoid clutter\n",
    "    plt.annotate(\n",
    "        f'{threshold:.2f}',\n",
    "        xy=(model_fpr[i*10], model_tpr[i*10]),\n",
    "        fontsize=8,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
    "plt.title('ROC Curve with Threshold Values', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Selection Strategy\n",
    "\n",
    "**Goal:** Find threshold where:\n",
    "- **TPR is HIGH** (maximize correct positive predictions)\n",
    "- **FPR is LOW** (minimize false alarms)\n",
    "\n",
    "**Decision depends on use case:**\n",
    "\n",
    "| Use Case | Priority | Threshold Strategy |\n",
    "|----------|----------|-------------------|\n",
    "| Cancer Detection | High TPR | Lower threshold (e.g., 0.3-0.4) |\n",
    "| Loan Approval | Low FPR | Higher threshold (e.g., 0.6-0.7) |\n",
    "| Email Spam | Balanced | Medium threshold (e.g., 0.5) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold using Youden's J statistic\n",
    "# J = TPR - FPR (maximize this)\n",
    "J = model_tpr - model_fpr\n",
    "optimal_idx = np.argmax(J)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal Threshold (Youden's J): {optimal_threshold:.4f}\")\n",
    "print(f\"TPR at optimal: {model_tpr[optimal_idx]:.4f}\")\n",
    "print(f\"FPR at optimal: {model_fpr[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to analyze threshold trade-offs\n",
    "threshold_df = pd.DataFrame({\n",
    "    'Threshold': thresholds,\n",
    "    'TPR': model_tpr,\n",
    "    'FPR': model_fpr,\n",
    "    'J_Statistic': J\n",
    "})\n",
    "\n",
    "# Show top 10 by J statistic\n",
    "print(\"Top 10 Threshold Candidates:\")\n",
    "print(threshold_df.nlargest(10, 'J_Statistic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimal threshold on ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot(model_fpr, model_tpr, marker='.', label='Logistic Regression', color='orange')\n",
    "plt.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model', color='blue')\n",
    "\n",
    "# Mark optimal point\n",
    "plt.scatter(model_fpr[optimal_idx], model_tpr[optimal_idx], \n",
    "            color='red', s=200, marker='*', \n",
    "            label=f'Optimal (threshold={optimal_threshold:.2f})', zorder=5)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve with Optimal Threshold', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Custom Threshold for Predictions\n",
    "\n",
    "> **Tip:** Use custom threshold by comparing probabilities instead of using `predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with custom threshold\n",
    "custom_threshold = optimal_threshold\n",
    "y_pred_custom = (model_prob >= custom_threshold).astype(int)\n",
    "\n",
    "print(f\"Using custom threshold: {custom_threshold:.4f}\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test_roc, y_pred_custom):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_roc, y_pred_custom))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_roc, y_pred_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs custom threshold\n",
    "y_pred_default = model_roc.predict(X_test_roc)  # uses 0.5 threshold\n",
    "\n",
    "print(\"COMPARISON: Default (0.5) vs Custom Threshold\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDefault Threshold (0.5):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_roc, y_pred_default):.4f}\")\n",
    "\n",
    "print(f\"\\nCustom Threshold ({custom_threshold:.4f}):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_roc, y_pred_custom):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "> **Remember:**\n",
    "> - `C` parameter: Inverse of regularization strength (higher C = less regularization)\n",
    "> - For small datasets: use `'liblinear'` solver\n",
    "> - For large datasets: use `'sag'` or `'saga'` solver\n",
    "> - Multi-class: Use `multi_class='ovr'` or `'multinomial'`\n",
    "> - Imbalanced data: Use `class_weight` parameter\n",
    "> - Threshold selection: Use ROC curve + domain expertise\n",
    "> - GridSearchCV: Exhaustive search (slow but thorough)\n",
    "> - RandomizedSearchCV: Random sampling (fast but may miss optimal)\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "1. **Not handling imbalanced data**  Use `class_weight`\n",
    "2. **Using default threshold blindly**  Analyze ROC curve\n",
    "3. **Ignoring regularization**  Use penalty parameter\n",
    "4. **Not doing cross-validation**  Use StratifiedKFold\n",
    "5. **Wrong solver for penalty**  Check compatibility\n",
    "\n",
    "### Solver Compatibility\n",
    "\n",
    "| Solver | L1 | L2 | ElasticNet | None |\n",
    "|--------|----|----|------------|------|\n",
    "| liblinear |  |  |  |  |\n",
    "| saga |  |  |  |  |\n",
    "| newton-cg |  |  |  |  |\n",
    "| lbfgs |  |  |  |  |\n",
    "| sag |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice Exercises\n",
    "\n",
    "**Try these to master the concepts:**\n",
    "\n",
    "1. Create an imbalanced dataset with 95:5 ratio and achieve >90% accuracy\n",
    "2. Use RandomizedSearchCV with 50 iterations and compare with GridSearchCV\n",
    "3. Implement 4-class classification and visualize confusion matrix\n",
    "4. Find optimal threshold for a healthcare problem (prioritize high recall)\n",
    "5. Compare performance with different regularization penalties\n",
    "\n",
    "**Good luck! Keep practicing! **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
